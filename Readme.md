[Where Are Large Language Models for Code Generation in GitHub?](#where-are-large-language-models-for-code-generation-in-gitHub)

- [Project Structure](#project-structure)
- [Get Started](#get-started)
- [License](#license)
- [Citation](#citation)

# Where Are Large Language Models for Code Generation in GitHub? <span id="where-are-large-language-models-for-code-generation-in-gitHub"></span>

In order to represent how developers actually utilize Large Language Models (LLMs)' code generation capabilities or clarify the characteristics of LLM-generated code in real-world development. We investigate the characteristics of LLM-generated code and its corresponding projects hosted on GitHub. Our main work is reflected in the following aspects:

* **Github Mining for GPT-generated Code**: 

  > 📢 "GPT-generated code" refers to code generated by GPT-3.5, GPT-4.0, ChatGPT, or Copilot.

  We utilize the official [GitHub REST API](https://docs.github.com/en/rest?apiVersion=2022-11-28) to automatically identify and collect files/projects containing GPT-generated code in the five common languages (Python, Java, C/C++, JavaScript, and TypeScript). Specifically, we conduct keyword searches, such as “generated by ChatGPT” and “generated by Copilot”, to locate GitHub code files that include such keywords, retaining only those files that contain GPT-generated code. The specific parameters for mining are as follows:

  * **Mined LLMs**:  ChatGPT and Copilot.

    > 📢 We identify all currently popular LLMs. We reveal that ChatGPT and Copilot are the most frequently utilized for generating code on GitHub. In contrast, all other LLMs (e.g. CodeLlama, CodeGen, StarCoder, LlaMA, DeepSeek, and Codex) have either not been used to generate code or have generated very little code on GitHub.

  * **Mined Languages**: Python, Java, C/C++, JavaScript, and TypeScript.

    > 📢 We select the [top 10 programming languages](https://octoverse.github.com/2022/top-programming-languages) from GitHub to download code in these programming languages, but we find that the number of GPT-generated code snippets in C#, PHP, Shell, and Ruby is less than 70, which may lead to statistically unreliable results. Consequently, we focus our analysis solely on GPT-generated code snippets in Python, Java, C/C++, JavaScript, and TypeScript on GitHub.

  * **Mined Keywords**: We select triplets of search keywords indicating code generated by ChatGPT/Copilot, which typically follow the pattern x+y+z, where x ∈ {generated, written, created, implemented, authored, coded}, y ∈ {by, through, using, via, with}, and z ∈ {ChatGPT, Copilot, GPT3, GPT4, GPT-3, and GPT-4}. Therefore, we use these triplets, such as “generated by ChatGPT,” as keywords to find and collect GPT-generated code on GitHub.

* **Analyze the Characteristics of GPT-Generated Code**:

  After mining, we only collect projects created from **February 2023** onwards and manually review whether the code is truly generated by GPT. From an initial collection of 1,417 Python, 261 Java, 342 C/C++, 693 JavaScript, and 209 TypeScript code files, we filter out files that do not contain GPT-generated code and ultimately identify **533** Python, **108** Java, **113** C/C++, **164** JavaScript, and **65** TypeScript projects, and corresponding **660** Python, **174** Java, **151** C/C++, **349** JavaScript, and **79** TypeScript files containing GPT-generated code. Before analysis, we processed the mined projects and code as follows:

  * **Retrieving code changes**: We leverage the project name and code file name as unique identifiers to retrieve the complete commit history for each code file. Based on the unique hash value of each commit, we utilize the GitHub REST API to obtain the content of the code file for its each code commit.
  * **Downloading code files and projects**: We download the file containing GPT-generated code and its corresponding project when the file was initially uploaded to GitHub.

  **Analysis of GPT-Generated Code.** We analyze the types of functions generated by GPT-generated code, the types of subsequent changes, and the types of bug fixes in the commit history. Additionally, we further extract and analyzed the comments. Furthermore, we use the [SonarQube tool](https://www.sonarsource.com/products/sonarqube/) to analyze various metrics of the GPT-generated code and its associated project, aiming to explore the characteristics of GPT-generated code, the project's features, and how GPT-generated code differs from human-written code within the same project. The operations we have performed as follows:
  
  - **Code categorization**: We classify GPT-generated code into different code types based on their functionalities, employing a combination of automated and manual methods. Please refer to [Code categorization](#Code categorization) for detailed instructions.
  - **Code change categorization**: We identify commit logs to determine bug-fix commits. When a developer submits a bug-fix commit, we check for code changes in the GPT-generated code snippets at the time of this commit. If there are code changes due to bug fixes, we analyze the bug-fix contents. Additionally, for non-bug-fix changes in GPT-generated code, we analyze the reasons behind the final code modifications. Please refer to [Code change categorization](#Code change categorization) for detailed instructions.
  - **Code analysis**: We utilize the static analysis tool [SonarQube](https://www.sonarsource.com/products/sonarqube/)  to analyze GPT-generated code and its corresponding projects. From SonarQube’s analysis results, we extract metrics such as LOC, number of files, and number of methods, as well as cyclomatic complexity and cognitive complexity.
  
  
  
## Project Structure

The main components of this code include a web crawler script for GitHub mining, as well as a data analysis module tailored for the GPT-generated code and its associated projects.

  **Project Structure Description**

  ```shell
  WhereCodeGenerationByLLM
      ├─crawled_data
      │    └─ChatGPT
      │        ├─python
      │        │    ├─created
      │        │    │    ├─code/
      │        │    │    ├─code_change/
      │        │    │    ├─project/
      │        │    │    └─Python_data.json
      │        │    ├─generated
      │        │    ├─coded
      │        │    ├─written
      │        │    └─implemented
      │        ├─java
      │        ├─javascript
      │        ├─typescript
      │        ├─cpp
      │        └─c
      ├─analysis_results
      │    └─ChatGPT
      │        ├─python
      │        │    ├─created
      │        │    ├─generated
      │        │    ├─coded
      │        │    ├─written
      │        │    ├─implemented
      │        │    ├─visualization
      │        │    └─Python_dataonly_first.csv
      │        ├─java
      │        ├─javascript
      │        ├─typescript
      │        ├─cpp
      │        └─visualization
      ├─analyze
      ├─crawl
      ├─config
      ├─utils
      ├─results-code-change-type
      ├─results-code-nochange-type
      └─wherecode
          └─cli.py
  ```

  > 📢 The related source data and analysis results for the GPT-generated code are stored in the path arrangement of `ChatGPT/{Language}/{Keyword}`. Note that path `ChatGPT` represents both ChatGPT and Copilot together, and we have not made a separation.

* **crawled_data**: 
  
  The `crawled_data` directory contains GitHub-mined metadata (e.g., `crawled_data/ChatGPT/python/created/Python_data.json`), downloaded files and projects that include GPT-generated code (e.g., `crawled_data/ChatGPT/python/created/project`), extracted GPT-generated code segments (e.g., `crawled_data/ChatGPT/python/created/code`), and the last modified GPT-generated code (e.g., `crawled_data/ChatGPT/python/created/code_change`).
  
  For example, `crawled_data/ChatGPT/python/created/Python_data.json` is the GitHub metadata of files and projects that may be related to GPT-generated code of Python language, which we have queried using the verb "created." The specific format of this `Python_data.json` is as shown in [ Format of mined json file](#crawl_format).
  
  **🔥Notice:** Due to the large size of the `crawled_data`, on GitHub, I have only uploaded the metadata for the 'created' crawls specific to the Python language. If you want to obtain all the crawled and source data content, we provide two versions available for download on OneDrive:
  
  1. **👉Full version**: Includes metadata from all GitHub crawls for five languages, downloaded projects, files, extracted GPT-generated code, and the last modified GPT-generated code. **After downloading, you can directly extract the files to overwrite the original `crawled_data` folder.**
  
     **[crawled_data.zip](https://1drv.ms/u/s!AoC1JXWLcNgVgWmwX_qKd4lXTFQ5)**
  
  2. 👉**Processed version**: Comprises three parts: source files containing GPT-generated code, extracted GPT-generated code, and the last modified GPT-generated code. **This content is primarily intended for users to perform secondary processing on the GPT-generated code.**
  
     **[gpt_code_snippets&files.zip](https://1drv.ms/u/s!AoC1JXWLcNgVgWqwX_qKd4lXTFQ5)**
  
* **analysis_results**: 
  
  The `analysis_results` directory contains the analysis results for each language, where `cpp` represents the combined analysis results for both C and C++ languages. The summary of the analysis results is stored in a CSV file, such as `analysis_results/ChatGPT/python/Python_dataonly_first.csv` (for details on the meaning of each field in the analysis results, see [Detailed Explanation]()). The `visualization` folder, on the other hand, contains some charts and statistical results.
  
* **results-code-change-type**: Contains the types of changes for each GPT-generated code snippets.
  
* **results-code-nochange-type**: Contains the functional types of each GPT-generated code snippets.
  
* **analyze**: Data Analysis and Visualization Module.
  
* **crawl**: GitHub Data Mining Module.
  
* **config**: Project Configuration Module.
  
* **utils**: Project Utils Module.
  
* **wherecode**: Command Line Configuration.
  
## Get Started

### 1. Download the project code

* Git clone

  ```bash
  git clone https://github.com/lkpttxg/WhereCodeGenerationByLLM.git
  ```

* [Download zip](https://github.com/lkpttxg/WhereCodeGenerationByLLM/archive/refs/heads/main.zip)

### 2. Environment Setup

We recommend that the runtime environment for the code should be `Python >= 3.9` with `pip` installed.

Then, set up the runtime environment by running the [requirements.txt](requirements.txt).

```bash
pip install -r requirements.txt
```

### 3. Command execution

You can start the main functionality of the code through the `command line`, or you can directly implement specific functions by calling script methods. We mainly introduce command-line startup. For specifics on invoking scripts, please refer to the source code.

* 🔥command-line startup:

  ```sh
  cd WhereCodeGenerationByLLM # Move to the root directory of the project
  
  python -m wherecode.cli ...
  ```

* import script method:

  ```python
  # You can import target method
  from analyze import analyze_gpt_generated_code_and_more, calculate_various_measures, ...
  from crawl.code_crawler import crawl_action
  from utils import read_xlsx_to_csv
  ...
  ```

#### 3.1 Overview

After you have completed the environment configuration installation, open the command console, and first switch to the root directory of the project.

```shell
cd WhereCodeGenerationByLLM
```

Then, start the command:

```sh
python -m wherecode.cli
```

All subcommands are as follows:

```bash
usage: cli.py [-h] {analyze,sum_res,stat-dist,stat-all,draw,mv,crawl,stat_type} ...

Command-line interface for running tasks.

positional arguments:
  {analyze,sum_res,stat-dist,stat-all,draw,mv,crawl,stat_type}
                        sub-command help
    analyze             Analyze github code generated by LLms.
    sum_res             Summarize the analysis results of different keywords.
    stat-dist           Statistics on the distribution of GPT-generated code characteristics in a particular language.
    stat-all            Statistics summary of GPT-generated code characteristics in different languages.
    draw                Draw violin plots for the 3 research questions (RQs).
    mv                  Read the results xlsx answered by gpt-4.0, move to the results statistics table csv of a certain language.
    crawl               Based on search keywords, obtain detailed information about files, projects, and repositories containing the keywords through precise searches on GitHub.   
    stat_type           Statistics of the code types, change types, and bug-fix types of GPT-generated code.

optional arguments:
  -h, --help            show this help message and exit
```

#### 3.2 Crawl Github

First, crawl possible files containing GPT-generated code through the [GitHub REST API](https://docs.github.com/en/rest?apiVersion=2022-11-28), then record various attributes of the projects they belong to and the commit history within the projects.

```bash
python cli.py crawl --token=<github token> --get_page_wait_time=<github url crawl interval> --get_info_wait_time=<file detail info interval>
```

eg: 

```
python cli.py crawl --token=xxxxxxxxx --get_page_wait_time=10 --get_info_wait_time=0.4
```

args:

* `--token`: Your github token.For details on how to obtain githubtoken, please refer to the website [GitHub Docs](https://docs.github.com/en).

* `--get_page_wait_time`: Before crawling starts, we need to generate the interface url according to the number of query files.Due to the relevant policies of github rest api, there are corresponding rate limits for different tokens. Please set the paging query rate according to your token permissions. The paging query interval for common users is 10s

* `--get_info_wait_time`:Interval for obtaining all information about a file.Due to the relevant policies of github rest api, there are corresponding rate limits for different tokens.In order to ensure the stable operation of the crawler, we choose 0.4s as the crawl interval.

🚧Please refer to the specific rate limit  [Rate limits for the REST API - GitHub Docs](https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28)

if you want to search other keywords please change the code of [crawl/search_word_list.py](crawl/search_word_list.py).

**Explanation of fields in the output JSON file:** <span id="crawl_format"></span>

> The final crawl results are uniformly stored in JSON format within folders differentiated by search keywords. The storage path is `WhereCodeGenerationByLLM/crawled_data/ChatGPT/{Language}/{Keyword}`.

```json
{
  // Project name
  "project_name": "librosa",
  // Project HTML URL
  "project_html_url": "https://github.com/librosa/librosa",
  // Number of contributors
  "contributor_num": "114",
  // Composition of main languages in the project
  "language_info": "Python98.2%MATLAB1.3%Other0.5%",
  // Repository star count
  "repo_star_num": 6689,
  // Repository fork count
  "repo_fork_num": 921,
  // Repository issue count
  "repo_issues_num": 55,
  // Repository watcher count
  "repo_watch_num": 137,
  // Main language of the repository
  "repo_main_langauge": "Python",
  // Repository description
  "repo_about": "Python library for audio and music analysis",
  // Repository creation date
  "repo_create_date": "2012-10-20T14:21:01Z",
  // Repository last update date
  "repo_update_date": "2024-04-27T23:00:08Z",
  // Repository tags
  "repo_tags": [
    "audio",
    "dsp",
    "librosa",
    "music",
    "python",
    "scipy"
  ],
  // List of all commit information for the repository (dates in descending order, newest date at the top)
  "repo_all_commit_info_list": [
    {
      // Commit hash
      "hash_code": "14746316d931e331d1b6686d60bb593312b1d29e",
      // Commit message
      "description": "librosa.cite() (#1829)\n\n* added librosa.cite()\r\n\r\n* bypass integrity check on version index\r\n\r\n* forgot typing on cite\r\n\r\n* styling\r\n\r\n* bumped smallest pooch to 1.1",
      // Commit date
      "date": "2024-04-03T20:11:11Z",
      // Link to get detailed commit information
      "link": "https://api.github.com/repos/librosa/librosa/git/commits/14746316d931e331d1b6686d60bb593312b1d29e",
      // Commit HTML URL
      "html_url": "https://github.com/librosa/librosa/commit/14746316d931e331d1b6686d60bb593312b1d29e",
      // Project download URL corresponding to this commit
      "project_download_url": "(Download URL for the project corresponding to this commit) https://api.github.com/repos/librosa/librosa/zipball/14746316d931e331d1b6686d60bb593312b1d29e"
    }
  ],
  // All commit information for the corresponding file (newest date at the top)
  "file_all_commit_info_list": [
    {
      // Commit hash
      "hash_code": "910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688",
      // File path relative to the project
      "file_path": "scripts/create_author_list.py",
      // Commit creation date
      "create_date": "2023-02-17T15:34:05Z",
      // Project download URL for the corresponding commit
      "project_download_url": "https://api.github.com/repos/librosa/librosa/zipball/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688",
      // File size in KB
      "size": 1304,
      // File download URL
      "download_url": "https://raw.githubusercontent.com/librosa/librosa/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688/scripts/create_author_list.py",
      // File HTML URL
      "html_url": "https://github.com/librosa/librosa/blob/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688/scripts/create_author_list.py",
      // File content
      "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -\n*-\n\"\"\"Script to retrieve contributors from Gi\ntHub API\n\nInitially generated by ChatGPT (Jan\n 30 version): https://help.openai.com/en/arti\ncles/6825453-chatgpt-release-notes\n\"\"\"\n\nimpor\nt requests\nimport operator\n\ndef get_contribut\nors(repo_owner, repo_name):\n    url = f\"https\n://api.github.com/repos/{repo_owner}/{repo_na\nme}/stats/contributors\"\n    contributors = {}\n\n\n    while url:\n        response = requests.\nget(url)\n\n        if response.status_code != \n200:\n            raise Exception(f\"Failed to \nretrieve contributors: {response.json()['mess\nage']}\")\n\n        for contributor in response\n.json():\n            contributions = contribu\ntor[\"weeks\"]\n            mod_lines = sum([wee\nk[\"d\"] + week[\"a\"] for week in contributions]\n)\n            contributors[contributor[\"autho\nr\"][\"login\"]] = mod_lines\n\n        if \"next\" \nin response.links:\n            url = response\n.links[\"next\"][\"url\"]\n        else:\n         \n   url = None\n\n    return sorted(contributors\n.items(), key=operator.itemgetter(1), reverse\n=True)\n\nif __name__ == \"__main__\":\n    repo_o\nwner = \"librosa\"\n    repo_name = \"librosa\"\n\n \n   contributors = get_contributors(repo_owner\n, repo_name)\n    for contributor, mod_lines i\nn contributors:\n        print(f\"{contributor:\n30s}|\\t{mod_lines:7d} lines added/deleted\")\n\n"
    }
  ]
}
```

## License

  Our code is licensed under the [Apache-2.0 license](LICENSE).

  

## Citation

  ```
  @article{yu2024large,
    title={Where Are Large Language Models for Code Generation on GitHub?},
    author={Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky Wai and Liu, Jin and Xia, Xin},
    journal={arXiv preprint arXiv:2406.19544},
    year={2024}
  }
  ```

  

  

  

  

  