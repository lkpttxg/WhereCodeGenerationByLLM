[Where Are Large Language Models for Code Generation in GitHub?](#where-are-large-language-models-for-code-generation-in-gitHub)

- [Project Structure](#project-structure)
- [Get Started](#get-started)
- [License](#license)
- [Citation](#citation)

# Where Are Large Language Models for Code Generation in GitHub? <span id="where-are-large-language-models-for-code-generation-in-gitHub"></span>

In order to represent how developers actually utilize Large Language Models (LLMs)' code generation capabilities or clarify the characteristics of LLM-generated code in real-world development. We investigate the characteristics of LLM-generated code and its corresponding projects hosted on GitHub. Our main work is reflected in the following aspects:

* **Github Mining for GPT-generated Code**: 

  > 📢 "GPT-generated code" refers to code generated by GPT-3.5, GPT-4.0, ChatGPT, or Copilot.

  We utilize the official [GitHub REST API](https://docs.github.com/en/rest?apiVersion=2022-11-28) to automatically identify and collect files/projects containing GPT-generated code in the five common languages (Python, Java, C/C++, JavaScript, and TypeScript). Specifically, we conduct keyword searches, such as “generated by ChatGPT” and “generated by Copilot”, to locate GitHub code files that include such keywords, retaining only those files that contain GPT-generated code. The specific parameters for mining are as follows:

  * **Mined LLMs**:  ChatGPT and Copilot.

    > 📢 We identify all currently popular LLMs. We reveal that ChatGPT and Copilot are the most frequently utilized for generating code on GitHub. In contrast, all other LLMs (e.g. CodeLlama, CodeGen, StarCoder, LlaMA, DeepSeek, and Codex) have either not been used to generate code or have generated very little code on GitHub.

  * **Mined Languages**: Python, Java, C/C++, JavaScript, and TypeScript.

    > 📢 We select the [top 10 programming languages](https://octoverse.github.com/2022/top-programming-languages) from GitHub to download code in these programming languages, but we find that the number of GPT-generated code snippets in C#, PHP, Shell, and Ruby is less than 70, which may lead to statistically unreliable results. Consequently, we focus our analysis solely on GPT-generated code snippets in Python, Java, C/C++, JavaScript, and TypeScript on GitHub.

  * **Mined Keywords**: We select triplets of search keywords indicating code generated by ChatGPT/Copilot, which typically follow the pattern x+y+z, where x ∈ {generated, written, created, implemented, authored, coded}, y ∈ {by, through, using, via, with}, and z ∈ {ChatGPT, Copilot, GPT3, GPT4, GPT-3, and GPT-4}. Therefore, we use these triplets, such as “generated by ChatGPT,” as keywords to find and collect GPT-generated code on GitHub.

* **Analyze the Characteristics of GPT-Generated Code**:

  After mining, we only collect projects created from **February 2023** onwards and manually review whether the code is truly generated by GPT. From an initial collection of 1,417 Python, 261 Java, 342 C/C++, 693 JavaScript, and 209 TypeScript code files, we filter out files that do not contain GPT-generated code and ultimately identify **533** Python, **108** Java, **113** C/C++, **164** JavaScript, and **65** TypeScript projects, and corresponding **660** Python, **174** Java, **151** C/C++, **349** JavaScript, and **79** TypeScript files containing GPT-generated code. Before analysis, we processed the mined projects and code as follows:

  * **Retrieving code changes**: We leverage the project name and code file name as unique identifiers to retrieve the complete commit history for each code file. Based on the unique hash value of each commit, we utilize the GitHub REST API to obtain the content of the code file for its each code commit.
  * **Downloading code files and projects**: We download the file containing GPT-generated code and its corresponding project when the file was initially uploaded to GitHub.

  **Analysis of GPT-Generated Code.** We analyze the types of functions generated by GPT-generated code, the types of subsequent changes, and the types of bug fixes in the commit history. Additionally, we further extract and analyzed the comments. Furthermore, we use the [SonarQube tool](https://www.sonarsource.com/products/sonarqube/) to analyze various metrics of the GPT-generated code and its associated project, aiming to explore the characteristics of GPT-generated code, the project's features, and how GPT-generated code differs from human-written code within the same project. The operations we have performed as follows:
  
  - **Code categorization**: We classify GPT-generated code into different code types based on their functionalities, employing a combination of automated and manual methods. Please refer to [Code categorization](#Code categorization) for detailed instructions.
  - **Code change categorization**: We identify commit logs to determine bug-fix commits. When a developer submits a bug-fix commit, we check for code changes in the GPT-generated code snippets at the time of this commit. If there are code changes due to bug fixes, we analyze the bug-fix contents. Additionally, for non-bug-fix changes in GPT-generated code, we analyze the reasons behind the final code modifications. Please refer to [Code change categorization](#Code change categorization) for detailed instructions.
  - **Code analysis**: We utilize the static analysis tool [SonarQube](https://www.sonarsource.com/products/sonarqube/)  to analyze GPT-generated code and its corresponding projects. From SonarQube’s analysis results, we extract metrics such as LOC, number of files, and number of methods, as well as cyclomatic complexity and cognitive complexity.
  
  
  
## Project Structure

The main components of this code include a web crawler script for GitHub mining, as well as a data analysis module tailored for the GPT-generated code and its associated projects.

  **Project Structure Description**

  ```shell
  WhereCodeGenerationByLLM
      ├─crawled_data
      │    └─ChatGPT
      │        ├─python
      │        │    ├─created
      │        │    │    ├─code/
      │        │    │    ├─code_change/
      │        │    │    ├─project/
      │        │    │    └─Python_data.json
      │        │    ├─generated
      │        │    ├─coded
      │        │    ├─written
      │        │    └─implemented
      │        ├─java
      │        ├─javascript
      │        ├─typescript
      │        ├─cpp
      │        └─c
      ├─analysis_results
      │    └─ChatGPT
      │        ├─python
      │        │    ├─created
      │        │    ├─generated
      │        │    ├─coded
      │        │    ├─written
      │        │    ├─implemented
      │        │    ├─visualization
      │        │    └─Python_dataonly_first.csv
      │        ├─java
      │        ├─javascript
      │        ├─typescript
      │        ├─cpp
      │        └─visualization
      ├─analyze
      ├─crawl
      ├─config
      ├─utils
      ├─results-code-change-type
      ├─results-code-nochange-type
      └─wherecode
          └─cli.py
  ```

  > 📢 The related source data and analysis results for the GPT-generated code are stored in the path arrangement of `ChatGPT/{Language}/{Keyword}`. Note that path `ChatGPT` represents both ChatGPT and Copilot together, and we have not made a separation.

* **crawled_data**: 
  
  The `crawled_data` directory contains GitHub-mined metadata (e.g., `crawled_data/ChatGPT/python/created/Python_data.json`), downloaded files and projects that include GPT-generated code (e.g., `crawled_data/ChatGPT/python/created/project`), extracted GPT-generated code segments (e.g., `crawled_data/ChatGPT/python/created/code`), and the last modified GPT-generated code (e.g., `crawled_data/ChatGPT/python/created/code_change`).
  
  For example, `crawled_data/ChatGPT/python/created/Python_data.json` is the GitHub metadata of files and projects that may be related to GPT-generated code of Python language, which we have queried using the verb "created." The specific format of this `Python_data.json` is as shown in [ Format of mined json file](#crawl_format).
  
  **🔥Notice:** Due to the large size of the `crawled_data`, on GitHub, I have only uploaded the metadata for the 'created' crawls specific to the Python language. If you want to obtain all the crawled and source data content, we provide two versions available for download on OneDrive:
  
  1. **👉Full version** <span id="full_version"></span>: Includes metadata from all GitHub crawls for five languages, downloaded projects, files, extracted GPT-generated code, and the last modified GPT-generated code. **After downloading, you can directly extract the files to overwrite the original `crawled_data` folder.**
  
     **[crawled_data.zip](https://1drv.ms/f/c/15d8708b7525b580/QoC1JXWLcNgggBXjAAAAAAAAsF_6ineJV0xUOQ)**
  
  2. 👉**Processed version**: Comprises three parts: source files containing GPT-generated code, extracted GPT-generated code, and the last modified GPT-generated code. **This content is primarily intended for users to perform secondary processing on the GPT-generated code.**
  
     **[gpt_code_snippets&files.zip](https://1drv.ms/f/c/15d8708b7525b580/QoC1JXWLcNgggBXjAAAAAAAAsF_6ineJV0xUOQ)**
  
* **analysis_results**: 
  
  The `analysis_results` directory contains the analysis results for each language, where `cpp` represents the combined analysis results for both C and C++ languages. The summary of the analysis results is stored in a CSV file, such as `analysis_results/ChatGPT/python/Python_dataonly_first.csv` (for details on the meaning of each field in the analysis results, see [Detailed Explanation](#csv_format)). The `visualization` folder, on the other hand, contains some charts and statistical results.
  
* **results-code-change-type**: Contains the types of changes for each GPT-generated code snippets.
  
* **results-code-nochange-type**: Contains the functional types of each GPT-generated code snippets.
  
* **analyze**: Data Analysis and Visualization Module.
  
* **crawl**: GitHub Data Mining Module.
  
* **config**: Project Configuration Module.
  
* **utils**: Project Utils Module.
  
* **wherecode**: Command Line Configuration.
  
## Get Started

### 1. Download the project code

* Git clone

  ```bash
  git clone https://github.com/lkpttxg/WhereCodeGenerationByLLM.git
  ```

* [Download zip](https://github.com/lkpttxg/WhereCodeGenerationByLLM/archive/refs/heads/main.zip)

### 2. Environment Setup

We recommend that the runtime environment for the code should be `Python >= 3.9` with `pip` installed.

Then, set up the runtime environment by running the [requirements.txt](requirements.txt).

```bash
pip install -r requirements.txt
```

### 3. Command execution

You can start the main functionality of the code through the `command line`, or you can directly implement specific functions by calling script methods. We mainly introduce command-line startup. For specifics on invoking scripts, please refer to the source code.

* 🔥command-line startup:

  ```sh
  cd WhereCodeGenerationByLLM # Move to the root directory of the project
  
  python -m wherecode.cli ...
  ```

* import script method:

  ```python
  # You can import target method
  from analyze import analyze_gpt_generated_code_and_more, calculate_various_measures, ...
  from crawl.code_crawler import crawl_action
  from utils import read_xlsx_to_csv
  ...
  ```

#### 3.1 Overview

After you have completed the environment configuration installation, open the command console, and first switch to the root directory of the project.

```shell
cd WhereCodeGenerationByLLM
```

Then, start the command:

```sh
python -m wherecode.cli
```

All subcommands are as follows:

```bash
usage: cli.py [-h] {analyze,sum_res,stat-dist,stat-all,draw,mv,crawl,stat_type} ...

Command-line interface for running tasks.

positional arguments:
  {analyze,sum_res,stat-dist,stat-all,draw,mv,crawl,stat_type}
                        sub-command help
    analyze             Analyze github code generated by LLms.
    sum_res             Summarize the analysis results of different keywords.
    stat-dist           Statistics on the distribution of GPT-generated code characteristics in a particular language.
    stat-all            Statistics summary of GPT-generated code characteristics in different languages.
    draw                Draw violin plots for the 3 research questions (RQs).
    mv                  Read the results xlsx answered by gpt-4.0, move to the results statistics table csv of a certain language.
    crawl               Based on search keywords, obtain detailed information about files, projects, and repositories containing the keywords through precise searches on GitHub.   
    stat_type           Statistics of the code types, change types, and bug-fix types of GPT-generated code.

optional arguments:
  -h, --help            show this help message and exit
```

#### 3.2 Crawl Github

First, crawl possible files containing GPT-generated code through the [GitHub REST API](https://docs.github.com/en/rest?apiVersion=2022-11-28), then record various attributes of the projects they belong to and the commit history within the projects.

```bash
python cli.py crawl --token=<github token> --get_page_wait_time=<github url crawl interval> --get_info_wait_time=<file detail info interval>
```

eg: 

```
python cli.py crawl --token=xxxxxxxxx --get_page_wait_time=10 --get_info_wait_time=0.4
```

args:

* `--token`: Your github token.For details on how to obtain githubtoken, please refer to the website [GitHub Docs](https://docs.github.com/en).

* `--get_page_wait_time`: Before crawling starts, we need to generate the interface url according to the number of query files.Due to the relevant policies of github rest api, there are corresponding rate limits for different tokens. Please set the paging query rate according to your token permissions. The paging query interval for common users is 10s

* `--get_info_wait_time`:Interval for obtaining all information about a file.Due to the relevant policies of github rest api, there are corresponding rate limits for different tokens.In order to ensure the stable operation of the crawler, we choose 0.4s as the crawl interval.

🚧Please refer to the specific rate limit  [Rate limits for the REST API - GitHub Docs](https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28)

if you want to search other keywords please change the code of [crawl/search_word_list.py](crawl/search_word_list.py).

**Explanation of fields in the output JSON file:** <span id="crawl_format"></span>

> The final crawl results are uniformly stored in JSON format within folders differentiated by search keywords. The storage path is `WhereCodeGenerationByLLM/crawled_data/ChatGPT/{Language}/{Keyword}`.

```json
{
  // Project name
  "project_name": "librosa",
  // Project HTML URL
  "project_html_url": "https://github.com/librosa/librosa",
  // Number of contributors
  "contributor_num": "114",
  // Composition of main languages in the project
  "language_info": "Python98.2%MATLAB1.3%Other0.5%",
  // Repository star count
  "repo_star_num": 6689,
  // Repository fork count
  "repo_fork_num": 921,
  // Repository issue count
  "repo_issues_num": 55,
  // Repository watcher count
  "repo_watch_num": 137,
  // Main language of the repository
  "repo_main_langauge": "Python",
  // Repository description
  "repo_about": "Python library for audio and music analysis",
  // Repository creation date
  "repo_create_date": "2012-10-20T14:21:01Z",
  // Repository last update date
  "repo_update_date": "2024-04-27T23:00:08Z",
  // Repository tags
  "repo_tags": [
    "audio",
    "dsp",
    "librosa",
    "music",
    "python",
    "scipy"
  ],
  // List of all commit information for the repository (dates in descending order, newest date at the top)
  "repo_all_commit_info_list": [
    {
      // Commit hash
      "hash_code": "14746316d931e331d1b6686d60bb593312b1d29e",
      // Commit message
      "description": "librosa.cite() (#1829)\n\n* added librosa.cite()\r\n\r\n* bypass integrity check on version index\r\n\r\n* forgot typing on cite\r\n\r\n* styling\r\n\r\n* bumped smallest pooch to 1.1",
      // Commit date
      "date": "2024-04-03T20:11:11Z",
      // Link to get detailed commit information
      "link": "https://api.github.com/repos/librosa/librosa/git/commits/14746316d931e331d1b6686d60bb593312b1d29e",
      // Commit HTML URL
      "html_url": "https://github.com/librosa/librosa/commit/14746316d931e331d1b6686d60bb593312b1d29e",
      // Project download URL corresponding to this commit
      "project_download_url": "(Download URL for the project corresponding to this commit) https://api.github.com/repos/librosa/librosa/zipball/14746316d931e331d1b6686d60bb593312b1d29e"
    }
  ],
  // All commit information for the corresponding file (newest date at the top)
  "file_all_commit_info_list": [
    {
      // Commit hash
      "hash_code": "910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688",
      // File path relative to the project
      "file_path": "scripts/create_author_list.py",
      // Commit creation date
      "create_date": "2023-02-17T15:34:05Z",
      // Project download URL for the corresponding commit
      "project_download_url": "https://api.github.com/repos/librosa/librosa/zipball/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688",
      // File size in KB
      "size": 1304,
      // File download URL
      "download_url": "https://raw.githubusercontent.com/librosa/librosa/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688/scripts/create_author_list.py",
      // File HTML URL
      "html_url": "https://github.com/librosa/librosa/blob/910d1bafd5ec39a0bb7ec04fd809c8ab15a6f688/scripts/create_author_list.py",
      // File content
      "content": "#!/usr/bin/env python\n# -*- encoding: utf-8 -\n*-\n\"\"\"Script to retrieve contributors from Gi\ntHub API\n\nInitially generated by ChatGPT (Jan\n 30 version): https://help.openai.com/en/arti\ncles/6825453-chatgpt-release-notes\n\"\"\"\n\nimpor\nt requests\nimport operator\n\ndef get_contribut\nors(repo_owner, repo_name):\n    url = f\"https\n://api.github.com/repos/{repo_owner}/{repo_na\nme}/stats/contributors\"\n    contributors = {}\n\n\n    while url:\n        response = requests.\nget(url)\n\n        if response.status_code != \n200:\n            raise Exception(f\"Failed to \nretrieve contributors: {response.json()['mess\nage']}\")\n\n        for contributor in response\n.json():\n            contributions = contribu\ntor[\"weeks\"]\n            mod_lines = sum([wee\nk[\"d\"] + week[\"a\"] for week in contributions]\n)\n            contributors[contributor[\"autho\nr\"][\"login\"]] = mod_lines\n\n        if \"next\" \nin response.links:\n            url = response\n.links[\"next\"][\"url\"]\n        else:\n         \n   url = None\n\n    return sorted(contributors\n.items(), key=operator.itemgetter(1), reverse\n=True)\n\nif __name__ == \"__main__\":\n    repo_o\nwner = \"librosa\"\n    repo_name = \"librosa\"\n\n \n   contributors = get_contributors(repo_owner\n, repo_name)\n    for contributor, mod_lines i\nn contributors:\n        print(f\"{contributor:\n30s}|\\t{mod_lines:7d} lines added/deleted\")\n\n"
    }
  ]
}
```

### 3.3 Analyze Crawled Data

For metadata extracted from GitHub, a series of related data mining tasks can be performed. We provide JSON data of GPT generation code in `Python` obtained by crawling using the `created` keyword in the code, stored in `WhereCodeGenerationByLLM/crawled_data/ChatGPT/python/created/Python_data.json`. Please refer to the [download link](#full_version) for all the crawled data.

We can analyze the mined data using the following command:

```sh
python -m wherecode.cli analyze [--llm={chatgpt}] --lang={python,java,javascript,typescript,c,cpp} --keyword={generated,created,implemented,written,coded} [--start=<START>] [--end=<END>]
                      [--extract] [--diff] [--file] [--project] [--commit] [--sonarqube]
```

eg:

```sh
python -m wherecode.cli analyze --lang=python --keyword=created
# or
python -m wherecode.cli analyze --lang python --keyword created
```

**optional arguments:**

* `-h, --help`            show this help message and exit
* `--llm {chatgpt} `      Default is ChatGPT
*  `--lang {python,java,javascript,typescript,c,cpp}` language
*  `--keyword {generated,created,implemented,written,coded}` keyword
*   `--start START`         The location where the analysis starts
*   `--end END`             The location where the analysis ends
*   `--file [FILE]`        Whether to download the files that contain the gpt-generated code in each commit. Default is False.
*   `--extract [EXTRACT]`   Whether to save the extracted code. Default is False.
*   `--diff [DIFF]`         Whether to perform a diff analysis on the code changes. Default is False.
*   `--project [PROJECT]`   Whether to download the entire project of the commit that contains the first gpt-generated code. Default is False.
*   `--commit [COMMIT]`     Whether to download each commit message. Default is False.
*   `--sonarqube [SONARQUBE]` Whether to analyze the gpt-generated code or project using SonarQube. Default is False.

⭐ **When running the analysis for the first time, we would like to include all optional analysis parameters.** This includes downloading the code and files generated by GPT, as well as downloading commit information and commit history. Additionally, it includes using `SonarQube` to analyze files and projects:

> 🚧 When you select the `--sonarqube` option, make sure you have set up `SonarQube analysis service` correctly on your server. Please refer to the [official tutorial](https://docs.sonarsource.com/sonarqube/latest/setup-and-upgrade/overview/) on setting up SonarQube services. Once you have set up the service, configure your SonarQube service connection information in [WhereCodeGenerationByLLM/config/const.py](config/const.py)
>
> ```python
> from enum import Enum
> 
> sonarqube_server = "http://localhost:9000"  # Your SonarQube server url
> sonarqube_token = "Your SonarQube login token"
> sonarqube_username = "Your SonarQube login username"
> sonarqube_password = "Your SonarQube login password"
> 
> ...
> ```

```sh
python -m wherecode.cli analyze --lang=python --keyword=created --file --extract --diff --project --commit --sonarqube
```

⭐**Output result:** <span id="csv_format"></span> 

For example, `--lang=python --keyword=created`. The analysis results of the keywords in this language are saved at the following location. :[WhereCodeGenerationByLLM/analysis_results/ChatGPT/python/created/loc_change/Python_data.csv](analysis_results/ChatGPT/python/created/loc_change/Python_data.csv)

Once all keywords of this language have been analyzed, we can execute the following command to merge all analysis results 

```sh
python -m wherecode.cli sum_res  [--llm={chatgpt}] --lang={python,java,javascript,typescript,c,cpp}
```

eg:

```sh
python -m wherecode.cli sum_res --lang=python
```

, and ultimately save them at the following location: [WhereCodeGenerationByLLM/analysis_results/ChatGPT/python/Python_data_all.csv](analysis_results/ChatGPT/python/Python_data_all.csv)

The saved result is a CSV file, with the interpretation of each field as follows: 

| Field Name                                  | Description                                                  |
| ------------------------------------------- | ------------------------------------------------------------ |
| `index`                                     | Index of the file containing GPT-generated code              |
| `project_name`                              | Project name                                                 |
| `create_time`                               | Project creation time                                        |
| `project_language`                          | Primary language of the project                              |
| `contributor`                               | Number of contributors                                       |
| `star`                                      | Number of stars                                              |
| `fork`                                      | Number of forks                                              |
| `issues`                                    | Number of issues                                             |
| `watch`                                     | Number of watchers                                           |
| `project_commits`                           | Number of commits in the project                             |
| `code_language`                             | Language of the GPT-generated code                           |
| `keyword_index`                             | Index of the search keyword in the file, e.g., "generated by chatgpt" at line 1 |
| `matched_keyword`                           | Matched keyword, e.g., "generated by chatgpt"                |
| `url_of_first_file_commit`                  | HTML URL of the first commit containing GPT-generated code   |
| `path_of_first_file_commit`                 | Relative path of the file in the first commit containing GPT-generated code |
| `final_change_commit_path`                  | Relative path of the file in the final commit containing GPT-generated code change |
| `code_granularity_data`                     | Granularity of the GPT-generated code, e.g., "statement", "method", "class" |
| `code_func_type`                            | Functional type of the GPT-generated code                    |
| `number_of_bug_or_vulnerability_all_commit` | Number of bug or vulnerability fixes in all project commits  |
| `comments`                                  | /                                                            |
| `start_end_line_data`                       | Start and end lines of the GPT-generated code in the file, inclusive |
| `first_loc`                                 | Number of lines of code when GPT-generated code was first committed |
| `final_start_end_line_data`                 | Start and end lines of the GPT-generated code in the file after the final change, inclusive |
| `final_loc_add`                             | Number of lines of code added in the final change            |
| `final_loc_minus`                           | Number of lines of code removed in the final change          |
| `final_change_type`                         | Type of the final change                                     |
| `test_code_data`                            | Whether the GPT-generated code is test code                  |
| `regular_expression_data`                   | Whether the GPT-generated code is a regular expression or input |
| `number_of_commits`                         | Number of commits in the file containing GPT-generated code  |
| `number_of_change_commit_to_first`          | Number of commits with changes to the GPT-generated code since the first commit |
| `change_commit_to_first_index`              | Index of commits with changes to the GPT-generated code since the first commit |
| `change_commit_to_first_hash`               | Hash of commits with changes to the GPT-generated code since the first commit |
| `change_commit_to_first_blocks`             | Similarity of changes to the GPT-generated code since the first commit |
| `number_of_all_change_commit`               | Total number of commits with changes to the GPT-generated code |
| `all_change_commit_index`                   | Index of all commits with changes to the GPT-generated code  |
| `all_change_commit_hash`                    | Hash of all commits with changes to the GPT-generated code   |
| `all_change_commit_blocks`                  | Similarity of all changes to the GPT-generated code          |
| `number_of_all_change_fix_commit`           | Rough number of bug or vulnerability fix commits for the GPT-generated code |
| `real_fixed_commit_number`                  | Actual number of bug or vulnerability fix commits for the GPT-generated code |
| `real_fixed_commit_hash`                    | Hash of all bug or vulnerability fix commits for the GPT-generated code |
| `real_fixed_commit_reason`                  | Reason for all bug or vulnerability fix commits for the GPT-generated code |
| `all_change_fix_commit_index`               | Rough index of bug or vulnerability fix commits for the GPT-generated code |
| `all_change_fix_commit_hash`                | Rough hash of bug or vulnerability fix commits for the GPT-generated code |
| `all_change_fix_commit_blocks`              | Rough similarity of all bug or vulnerability fix commits for the GPT-generated code |
| `project_lines`                             | Number of lines in the project                               |
| `project_locs`                              | Number of lines of code in the project                       |
| `project_statements`                        | Number of statements in the project                          |
| `project_functions`                         | Number of functions in the project                           |
| `project_classes`                           | Number of classes in the project                             |
| `project_files`                             | Number of files in the project                               |
| `project_density_comments`                  | Comment density in the project                               |
| `project_comments`                          | Number of comment lines in the project                       |
| `project_duplicated_lines`                  | Number of duplicated lines in the project                    |
| `project_duplicated_blocks`                 | Number of duplicated blocks in the project                   |
| `project_duplicated_files`                  | Number of duplicated files in the project                    |
| `project_duplicated_lines_density`          | Density of duplicated lines in the project                   |
| `project_vulnerability`                     | Number of vulnerabilities in the project                     |
| `project_bugs`                              | Number of bugs in the project                                |
| `project_code_smells`                       | Number of code smells in the project                         |
| `project_sqale_index`                       | Technical debt index of the project                          |
| `project_sqale_debt_ratio`                  | Technical debt ratio of the project                          |
| `project_complexity_all`                    | Overall cyclomatic complexity of the project                 |
| `project_cognitive_complexity_all`          | Overall cognitive complexity of the project                  |
| `project_complexity_mean_method`            | Average cyclomatic complexity per method in the project      |
| `project_cognitive_complexity_mean_method`  | Average cognitive complexity per method in the project       |
| `code_lines`                                | Number of lines in the code                                  |
| `code_locs`                                 | Number of lines of code in the code                          |
| `code_statements`                           | Number of statements in the code                             |
| `code_functions`                            | Number of functions in the code                              |
| `code_classes`                              | Number of classes in the code                                |
| `code_files`                                | Number of files in the code                                  |
| `code_density_comments`                     | Comment density in the code                                  |
| `code_comments`                             | Number of comment lines in the code                          |
| `code_duplicated_lines`                     | Number of duplicated lines in the code                       |
| `code_duplicated_blocks`                    | Number of duplicated blocks in the code                      |
| `code_duplicated_files`                     | Number of duplicated files in the code                       |
| `code_duplicated_lines_density`             | Density of duplicated lines in the code                      |
| `code_code_smells`                          | Number of code smells in the code                            |
| `code_sqale_index`                          | Technical debt index of the code                             |
| `code_sqale_debt_ratio`                     | Technical debt ratio of the code                             |
| `code_complexity_all`                       | Overall cyclomatic complexity of the code                    |
| `code_cognitive_complexity_all`             | Cognitive complexity of the code                             |
| `code_complexity_mean_method`               | Average cyclomatic complexity per method in the code         |
| `code_cognitive_complexity_mean_method`     | Average cognitive complexity per method in the code          |


## License

  Our code is licensed under the [Apache-2.0 license](LICENSE).

## Citation

  ```
  @article{yu2024large,
    title={Where Are Large Language Models for Code Generation on GitHub?},
    author={Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky Wai and Liu, Jin and Xia, Xin},
    journal={arXiv preprint arXiv:2406.19544},
    year={2024}
  }
  ```

  

  

  

  

  
